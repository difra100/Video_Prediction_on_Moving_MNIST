{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning course 2022-2023\n",
    "## Project: Video Prediction on Moving MNIST\n",
    "### Project Contributors\n",
    "* Mattia Castelmare, 1815675\n",
    "* Andrea Giuseppe Di Francesco, 1836928\n",
    "* Enrico Fazzi, 2003876"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/tychovdo/MovingMNIST.git\n",
    "# !pip3 install pytorch-lightning==1.5.10\n",
    "# !pip3 install torchvision\n",
    "# !pip3 install matplotlib\n",
    "# !pip install wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "### wandb codes ###\n",
    "from MovingMNIST.MovingMNIST import *\n",
    "import wandb\n",
    "#####################\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning import Trainer\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import rc\n",
    "rc('animation', html='jshtml')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video(tensor):\n",
    "    ''' This function display a video, given a torch tensor (source: https://stackoverflow.com/questions/67261108/how-to-display-a-video-in-colab-using-a-pytorch-tensor-of-rgb-image-arrays)\n",
    "        INPUT: tensor (Frames x Channels x Height x Width) \n",
    "        OUTPUT: Display an animation '''\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    imgs = tensor\n",
    "    imgs = imgs.permute(0, 2, 3, 1)  # Permuting to (Bx)HxWxC format\n",
    "    frames = [[ax.imshow(imgs[i], cmap='gray')] for i in range(len(imgs))]\n",
    "\n",
    "    ani = animation.ArtistAnimation(fig, frames)\n",
    "    return ani\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    ''' This function is used within the Dataloader pytorch utility. \n",
    "        INPUT: batch: List of tuples of tensor, each tuple contains the input video and the target video, \n",
    "        OUTPUT: A tuple (input, gt), s.t. both of them has dimension (B x T x C x H x W)'''\n",
    "\n",
    "    list_tuples = list(map(lambda x: (torch.reshape(x[0], (1, x[0].shape[0], 1, x[0].shape[1], x[0].shape[2])), torch.reshape(\n",
    "        x[1], (1, x[1].shape[0], 1, x[1].shape[1], x[1].shape[2]))), batch))\n",
    "\n",
    "    input = list_tuples[0][0]\n",
    "    gt = list_tuples[0][1]\n",
    "\n",
    "    for i in range(1, len(list_tuples)):\n",
    "        input = torch.cat((input, list_tuples[i][0]), dim=0)\n",
    "        gt = torch.cat((gt, list_tuples[i][1]), dim=0)\n",
    "\n",
    "    return (input.type(torch.FloatTensor), gt.type(torch.FloatTensor))\n",
    "\n",
    "\n",
    "def save_model(model, loss, path):\n",
    "  # This function is a customized in order to save a pytorch model.\n",
    "    checkpoint = {'model_state': model.state_dict(),\n",
    "                  'loss': loss}\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "def load_model(path, model, device):\n",
    "  # This function loads a pytorch model from a path.\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def kronecker_product(dyn, static):\n",
    "\n",
    "    out = torch.zeros(static.shape)\n",
    "    for i in range(out.shape[0]):\n",
    "        r = torch.stack([torch.kron(ai, bi)\n",
    "                        for ai, bi in zip(dyn[i], static[i])], dim=0)\n",
    "        out[i, :, :, :] = r\n",
    "    return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_train = True\n",
    "shuffle_test = False\n",
    "download = False\n",
    "\n",
    "### wandb codes ###\n",
    "wb = True\n",
    "project_name = \"VP_on_MMNIST\"\n",
    "if wb:\n",
    "    wandb.login()\n",
    "###################\n",
    "\n",
    "SEED = 2812023\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch lightning DataModule:\n",
    "* Set the MMNIST Dataset object and Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pl_Dataset(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "\n",
    "        self.bs = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit':\n",
    "            self.train_set = MovingMNIST(root=root,\n",
    "                                         train=True,\n",
    "                                         download=download)\n",
    "        elif stage == 'test':\n",
    "            self.test_set = MovingMNIST(root=root,\n",
    "                                        train=False,\n",
    "                                        download=download)\n",
    "\n",
    "    def train_dataloader(self, *args, **kwargs):\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=self.train_set,\n",
    "            batch_size=self.bs,\n",
    "            shuffle=shuffle_train,\n",
    "            collate_fn=collate)\n",
    "\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self, *args, **kwargs):\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            dataset=self.test_set,\n",
    "            batch_size=self.bs,\n",
    "            shuffle=shuffle_test,\n",
    "            collate_fn=collate)\n",
    "\n",
    "        return test_loader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf = 'simvp'\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "lr = 1e-2\n",
    "wd = 5e-2\n",
    "\n",
    "tau = 1e-1  # KL Divergency parameter\n",
    "\n",
    "\n",
    "### Vanilla hyperparameters ###\n",
    "if model_conf == 'random':\n",
    "    hyperparameters = {'lr': lr,\n",
    "                       'wd': wd,\n",
    "                       'epochs': 100,\n",
    "                       'batch_size': batch_size\n",
    "                       }\n",
    "elif model_conf == 'simvp':\n",
    "    hyperparameters = {'lr': lr,\n",
    "                       'wd': wd,\n",
    "                       'epochs': 100,\n",
    "                       'batch_size': batch_size,\n",
    "                       'CNN': {'input': 1,\n",
    "                               'hidden_S': 64,  # Output channels in Encoder / Input channel in Decoder\n",
    "                               'output_T': 512,  # Output channels in Inception, Hidden C. == Output channels // 2\n",
    "                               'ksize': 3,\n",
    "                               'Ns': 4,\n",
    "                               'Nt': 3}\n",
    "                       }\n",
    "\n",
    "\n",
    "data = pl_Dataset(batch_size)\n",
    "\n",
    "data.setup(stage='fit')\n",
    "data.setup(stage='test')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our VanillaED (Enry and Matty)\n",
    "* Implementation of a Vanilla Encoder Decoder architecture with a skip connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VEncoder(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, padding, stride, n_layers):\n",
    "        super(VEncoder, self).__init__()\n",
    "        self.conv_layers = []\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=input_channels, out_channels=output_channels,\n",
    "                              kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "\n",
    "        self.conv_layers.append(self.conv)\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels=output_channels, out_channels=output_channels,\n",
    "                                    kernel_size=kernel_size, padding=padding, stride=stride))\n",
    "            self.conv_layers.append(nn.ReLU())\n",
    "\n",
    "        self.enc = nn.Sequential(*self.conv_layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        skip = self.enc[0](input)\n",
    "        return self.enc(input), skip\n",
    "\n",
    "\n",
    "class VDecoder(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, padding, stride, n_layers, output_padding):\n",
    "        super(VDecoder, self).__init__()\n",
    "\n",
    "        self.trans_conv_layers = []\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            self.trans_conv_layers.append(nn.ConvTranspose2d(in_channels=input_channels, out_channels=input_channels,\n",
    "                                          kernel_size=kernel_size, padding=padding, stride=stride, output_padding=output_padding))\n",
    "            self.trans_conv_layers.append(nn.ReLU())\n",
    "\n",
    "        self.dec = nn.Sequential(*self.trans_conv_layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.dec(input)\n",
    "\n",
    "\n",
    "class VEncoderDecoder(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, padding, stride, n_layers, output_padding):\n",
    "        super(VEncoderDecoder, self).__init__()\n",
    "\n",
    "        self.encoder = VEncoder(input_channels=input_channels, output_channels=output_channels,\n",
    "                                kernel_size=kernel_size, padding=padding, stride=stride, n_layers=n_layers)\n",
    "        self.decoder = VDecoder(input_channels=output_channels, output_channels=input_channels, kernel_size=kernel_size,\n",
    "                                padding=padding, stride=stride, n_layers=n_layers, output_padding=output_padding)\n",
    "\n",
    "        self.num_layers = n_layers\n",
    "\n",
    "        self.final = nn.ConvTranspose2d(in_channels=2*output_channels, out_channels=input_channels, kernel_size=kernel_size, padding=padding, stride=stride, output_padding=output_padding)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output, skip = self.encoder(input)\n",
    "        for i in range(self.num_layers):\n",
    "            output = self.decoder(output)\n",
    "\n",
    "        output = torch.cat((skip, output), dim = 1)\n",
    "        output = self.final(output)\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimVP\n",
    "* Implementation of the SimVP architecture, the details are available at https://arxiv.org/pdf/2206.05099.pdf,\n",
    "* Encoder (CNN) +  Translator (Inception) + Decoder (CNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_generator(N, reverse=False):\n",
    "    strides = [1, 2]*10\n",
    "    if reverse:\n",
    "        return list(reversed(strides[:N]))\n",
    "    else:\n",
    "        return strides[:N]\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, inputC, outputC, stride=None, padding='same', transpose=False, groups=8, kernel=hyperparameters['CNN']['ksize']):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        if not transpose:\n",
    "            self.conv = nn.Conv2d(\n",
    "                inputC, outputC, kernel_size=kernel, padding=padding, stride=stride)\n",
    "        else:\n",
    "            self.conv = nn.ConvTranspose2d(\n",
    "                inputC, outputC, kernel, stride=stride, padding=1, output_padding=stride // 2)\n",
    "\n",
    "        self.layernorm = nn.GroupNorm(groups, outputC)\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.leaky(self.layernorm(self.conv(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderSimVP(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers):\n",
    "        super(EncoderSimVP, self).__init__()\n",
    "        strides = stride_generator(n_layers)\n",
    "        layers = [ConvBlock(hyperparameters['CNN']['input'], hyperparameters['CNN']\n",
    "                            ['hidden_S'], groups=2, stride=strides[0], padding=1)]\n",
    "\n",
    "        for layer in range(1, n_layers):\n",
    "            stride = strides[layer]\n",
    "            layers.append(ConvBlock(\n",
    "                hyperparameters['CNN']['hidden_S'], hyperparameters['CNN']['hidden_S'], groups=2, stride=stride, padding=1))\n",
    "\n",
    "        self.enc = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C, H, W = x.shape\n",
    "\n",
    "        x = x.reshape((B*T, C, H, W))\n",
    "\n",
    "        skip = self.enc[0](x)\n",
    "        x = self.enc(x)\n",
    "\n",
    "        # x = x.reshape((B, T, hyperparameters['CNN']['hidden_S'], H, W))\n",
    "\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class DecoderSimVP(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers):\n",
    "        super(DecoderSimVP, self).__init__()\n",
    "        strides = stride_generator(n_layers, reverse=True)\n",
    "        layers = []\n",
    "        for layer in range(n_layers-1):\n",
    "            stride = strides[layer]\n",
    "            layers.append(ConvBlock(hyperparameters['CNN']['hidden_S'], hyperparameters['CNN']\n",
    "                          ['hidden_S'], transpose=True, groups=2, stride=stride, padding=1))\n",
    "\n",
    "        layers.append(ConvBlock(2*hyperparameters['CNN']['hidden_S'], hyperparameters['CNN']\n",
    "                      ['hidden_S'], transpose=True, groups=2, stride=strides[-1], padding=1))\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            hyperparameters['CNN']['hidden_S'], hyperparameters['CNN']['input'], 1)\n",
    "\n",
    "        self.dec = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "\n",
    "        for l_idx in range(len(self.dec)-1):\n",
    "            x = self.dec[l_idx](x)\n",
    "\n",
    "        x = self.dec[-1](torch.cat((x, skip), dim=1))\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "\n",
    "    def __init__(self, inputC, outputC, kernel_list=[3, 5, 7, 11], groups=8):\n",
    "        super(InceptionModule, self).__init__()\n",
    "\n",
    "        hiddenC = outputC // 2\n",
    "\n",
    "        self.Conv = nn.Conv2d(\n",
    "            inputC, hiddenC, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        for kernel in kernel_list:\n",
    "            layers.append(ConvBlock(hiddenC, outputC, groups=4,\n",
    "                          stride=1, padding=kernel // 2, kernel=kernel))\n",
    "\n",
    "        self.ConvParallel = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.Conv(input)\n",
    "\n",
    "        output = 0\n",
    "\n",
    "        for conv in self.ConvParallel:\n",
    "\n",
    "            output += conv(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Translator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers, inputC):\n",
    "        super(Translator, self).__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        enc_layers = [InceptionModule(\n",
    "            inputC, hyperparameters['CNN']['output_T'])]\n",
    "\n",
    "        for inc_layer in range(n_layers-1):\n",
    "            enc_layers.append(InceptionModule(\n",
    "                hyperparameters['CNN']['output_T'], hyperparameters['CNN']['output_T']))\n",
    "\n",
    "        enc_layers.append(InceptionModule(\n",
    "            hyperparameters['CNN']['output_T'], hyperparameters['CNN']['output_T']))\n",
    "\n",
    "        self.encoder = nn.Sequential(*enc_layers)\n",
    "\n",
    "        dec_layers = [InceptionModule(\n",
    "            hyperparameters['CNN']['output_T'], hyperparameters['CNN']['output_T'])]\n",
    "\n",
    "        for inc_layers in range(n_layers-1):\n",
    "            dec_layers.append(InceptionModule(\n",
    "                2*hyperparameters['CNN']['output_T'], hyperparameters['CNN']['output_T']))\n",
    "        dec_layers.append(InceptionModule(\n",
    "            2*hyperparameters['CNN']['output_T'], inputC))\n",
    "\n",
    "        self.decoder = nn.Sequential(*dec_layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        B, T, C, H, W = input.shape\n",
    "        input = input.reshape((B, T*C, H, W))\n",
    "\n",
    "        enc_feat = []\n",
    "        mid_feat = input\n",
    "\n",
    "        for encoder_layer in self.encoder:\n",
    "            mid_feat = encoder_layer(mid_feat)\n",
    "            enc_feat.append(mid_feat)\n",
    "\n",
    "        output = self.decoder[0](mid_feat)\n",
    "\n",
    "        for l_idx in range(1, self.n_layers+1):\n",
    "\n",
    "            input = torch.cat((output, enc_feat[-l_idx]), dim=1)\n",
    "            output = self.decoder[l_idx](input)\n",
    "\n",
    "        output = output.reshape((B, T, C, H, W))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class SimVP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimVP, self).__init__()\n",
    "\n",
    "        self.encoder = EncoderSimVP(hyperparameters['CNN']['Ns'])\n",
    "        self.translator = Translator(\n",
    "            hyperparameters['CNN']['Nt'], 10*hyperparameters['CNN']['hidden_S'])\n",
    "        self.decoder = DecoderSimVP(hyperparameters['CNN']['Ns'])\n",
    "\n",
    "    def forward(self, input):\n",
    "        B, T, C, H, W = input.shape\n",
    "\n",
    "        output, skip = self.encoder(input)\n",
    "        _, C_, H_, W_ = output.shape\n",
    "        output = output.view(B, T, C_, H_, W_)\n",
    "\n",
    "        output = self.translator(output)\n",
    "\n",
    "        output = output.reshape((B*T, C_, H_, W_))\n",
    "\n",
    "        Y = self.decoder(output, skip)\n",
    "\n",
    "        Y = Y.reshape((B, T, C, H, W))\n",
    "\n",
    "        return Y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, transpose=False, act_norm=False):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.act_norm = act_norm\n",
    "        if not transpose:\n",
    "            self.conv = nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        else:\n",
    "            self.conv = nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=stride // 2)\n",
    "        self.norm = nn.GroupNorm(2, out_channels)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        if self.act_norm:\n",
    "            y = self.act(self.norm(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class ConvSC(nn.Module):\n",
    "    def __init__(self, C_in, C_out, stride, transpose=False, act_norm=True):\n",
    "        super(ConvSC, self).__init__()\n",
    "        if stride == 1:\n",
    "            transpose = False\n",
    "        self.conv = BasicConv2d(C_in, C_out, kernel_size=3, stride=stride,\n",
    "                                padding=1, transpose=transpose, act_norm=act_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GroupConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, act_norm=False):\n",
    "        super(GroupConv2d, self).__init__()\n",
    "        self.act_norm = act_norm\n",
    "        if in_channels % groups != 0:\n",
    "            groups = 1\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
    "                              stride=stride, padding=padding, groups=groups)\n",
    "        self.norm = nn.GroupNorm(groups, out_channels)\n",
    "        self.activate = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        if self.act_norm:\n",
    "            y = self.activate(self.norm(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, C_in, C_hid, C_out, incep_ker=[3, 5, 7, 11], groups=8):\n",
    "        super(Inception, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(C_in, C_hid, kernel_size=1, stride=1, padding=0)\n",
    "        layers = []\n",
    "        for ker in incep_ker:\n",
    "            layers.append(GroupConv2d(C_hid, C_out, kernel_size=ker,\n",
    "                          stride=1, padding=ker//2, groups=groups, act_norm=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        y = 0\n",
    "        for layer in self.layers:\n",
    "            y += layer(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "def stride_generator(N, reverse=False):\n",
    "    strides = [1, 2]*10\n",
    "    if reverse:\n",
    "        return list(reversed(strides[:N]))\n",
    "    else:\n",
    "        return strides[:N]\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, C_in, C_hid, N_S):\n",
    "        super(Encoder, self).__init__()\n",
    "        strides = stride_generator(N_S)\n",
    "        self.enc = nn.Sequential(\n",
    "            ConvSC(C_in, C_hid, stride=strides[0]),\n",
    "            *[ConvSC(C_hid, C_hid, stride=s) for s in strides[1:]]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # B*4, 3, 128, 128\n",
    "        enc1 = self.enc[0](x)\n",
    "        latent = enc1\n",
    "        for i in range(1, len(self.enc)):\n",
    "            latent = self.enc[i](latent)\n",
    "\n",
    "        return latent, enc1\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, C_hid, C_out, N_S):\n",
    "        super(Decoder, self).__init__()\n",
    "        strides = stride_generator(N_S, reverse=True)\n",
    "        self.dec = nn.Sequential(\n",
    "            *[ConvSC(C_hid, C_hid, stride=s, transpose=True)\n",
    "              for s in strides[:-1]],\n",
    "            ConvSC(2*C_hid, C_hid, stride=strides[-1], transpose=True)\n",
    "        )\n",
    "        self.readout = nn.Conv2d(C_hid, C_out, 1)\n",
    "\n",
    "    def forward(self, hid, enc1=None):\n",
    "        for i in range(0, len(self.dec)-1):\n",
    "            hid = self.dec[i](hid)\n",
    "        Y = self.dec[-1](torch.cat([hid, enc1], dim=1))\n",
    "        Y = self.readout(Y)\n",
    "        return Y\n",
    "\n",
    "\n",
    "class Mid_Xnet(nn.Module):\n",
    "    def __init__(self, channel_in, channel_hid, N_T, incep_ker=[3, 5, 7, 11], groups=8):\n",
    "        super(Mid_Xnet, self).__init__()\n",
    "\n",
    "        self.N_T = N_T\n",
    "        enc_layers = [Inception(\n",
    "            channel_in, channel_hid//2, channel_hid, incep_ker=incep_ker, groups=groups)]\n",
    "        for i in range(1, N_T-1):\n",
    "            enc_layers.append(Inception(\n",
    "                channel_hid, channel_hid//2, channel_hid, incep_ker=incep_ker, groups=groups))\n",
    "        enc_layers.append(Inception(channel_hid, channel_hid //\n",
    "                          2, channel_hid, incep_ker=incep_ker, groups=groups))\n",
    "\n",
    "        dec_layers = [Inception(\n",
    "            channel_hid, channel_hid//2, channel_hid, incep_ker=incep_ker, groups=groups)]\n",
    "        for i in range(1, N_T-1):\n",
    "            dec_layers.append(Inception(\n",
    "                2*channel_hid, channel_hid//2, channel_hid, incep_ker=incep_ker, groups=groups))\n",
    "        dec_layers.append(Inception(2*channel_hid, channel_hid //\n",
    "                          2, channel_in, incep_ker=incep_ker, groups=groups))\n",
    "\n",
    "        self.enc = nn.Sequential(*enc_layers)\n",
    "        self.dec = nn.Sequential(*dec_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.reshape(B, T*C, H, W)\n",
    "\n",
    "        # encoder\n",
    "        skips = []\n",
    "        z = x\n",
    "        for i in range(self.N_T):\n",
    "            z = self.enc[i](z)\n",
    "            if i < self.N_T - 1:\n",
    "                skips.append(z)\n",
    "\n",
    "        # decoder\n",
    "        z = self.dec[0](z)\n",
    "        for i in range(1, self.N_T):\n",
    "            z = self.dec[i](torch.cat([z, skips[-i]], dim=1))\n",
    "\n",
    "        y = z.reshape(B, T, C, H, W)\n",
    "        return y\n",
    "\n",
    "\n",
    "class SimVP(nn.Module):\n",
    "    def __init__(self, shape_in, hid_S=64, hid_T=256, N_S=4, N_T=8, incep_ker=[3, 5, 7, 11], groups=8):\n",
    "        super(SimVP, self).__init__()\n",
    "        T, C, H, W = shape_in\n",
    "        self.enc = Encoder(C, hid_S, N_S)\n",
    "        self.hid = Mid_Xnet(T*hid_S, hid_T, N_T, incep_ker, groups)\n",
    "        self.dec = Decoder(hid_S, C, N_S)\n",
    "\n",
    "    def forward(self, x_raw):\n",
    "        B, T, C, H, W = x_raw.shape\n",
    "        x = x_raw.view(B*T, C, H, W)\n",
    "\n",
    "        embed, skip = self.enc(x)\n",
    "        _, C_, H_, W_ = embed.shape\n",
    "\n",
    "        z = embed.view(B, T, C_, H_, W_)\n",
    "        hid = self.hid(z)\n",
    "        hid = hid.reshape(B*T, C_, H_, W_)\n",
    "\n",
    "        Y = self.dec(hid, skip)\n",
    "        Y = Y.reshape(B, T, C, H, W)\n",
    "        return Y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAU Module\n",
    "* Implementation of th Temporal Attention Unit architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dyn_attention(nn.Module):\n",
    "    def __init__(self, input, n_layers, hidden_dim, H_final, W_final, hidden_act=True): \n",
    "        super(Dyn_attention, self).__init__()\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((H_final, W_final))\n",
    "        layers = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test = self.avg_pool(input)\n",
    "            flat = F.torch.flatten(test)\n",
    "            input_ftrs = flat.shape[0]\n",
    "\n",
    "        layers.append(nn.Linear(\n",
    "            in_features=input_ftrs, out_features=hidden_dim, bias=True))\n",
    "\n",
    "        if hidden_act:\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(in_features=hidden_dim,\n",
    "                          out_features=hidden_dim, bias=True))\n",
    "            if hidden_act:\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        layers.append(nn.Linear(in_features=hidden_dim,\n",
    "                      out_features=input_ftrs, bias=True))\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.MLP = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, H):\n",
    "        x = self.avg_pool(H)\n",
    "        B, TC, _, _ = x.shape\n",
    "\n",
    "        x = F.torch.flatten(x)\n",
    "        x = self.MLP(x).view(B, TC, 1, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Static_attention(nn.Module):\n",
    "    def __init__(self, input, small_kern, dilatation, n_layers):\n",
    "        super(Static_attention, self).__init__()\n",
    "\n",
    "        B, TC, H, W = input.shape\n",
    "\n",
    "        depth_conv = []\n",
    "        dilat_conv = []\n",
    "        last_conv = []\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            depth_conv.append(nn.Conv2d(\n",
    "                in_channels=TC, out_channels=TC, kernel_size=small_kern, padding='same', groups=TC))\n",
    "            dilat_conv.append(nn.Conv2d(\n",
    "                in_channels=TC, out_channels=TC, kernel_size=small_kern, padding='same', dilation=dilatation, groups=TC))\n",
    "            last_conv.append(\n",
    "                nn.Conv2d(in_channels=TC, out_channels=TC, kernel_size=1, padding='same'))\n",
    "\n",
    "        self.depth_conv = nn.Sequential(*depth_conv)\n",
    "        self.dilat_conv = nn.Sequential(*dilat_conv)\n",
    "        self.last_conv = nn.Sequential(*last_conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depth_conv(x)\n",
    "        x = self.dilat_conv(x)\n",
    "        x = self.last_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TAU(nn.Module):\n",
    "    def __init__(self, input, hidden_dim, small_kern, dilatation, num_layers, H_final=1, W_final=1):\n",
    "        super(TAU, self).__init__()\n",
    "\n",
    "        self.DA_module = Dyn_attention(\n",
    "            input=input, H_final=H_final, W_final=W_final, hidden_dim=hidden_dim, n_layers=num_layers, hidden_act=True)\n",
    "        self.SA_module = Static_attention(\n",
    "            input=input, small_kern=small_kern, dilatation=dilatation, n_layers=num_layers)\n",
    "\n",
    "    def forward(self, H):\n",
    "        dynamic = self.DA_module(H)\n",
    "        static = self.SA_module(H)\n",
    "        res = kronecker_product(dynamic, static)\n",
    "        res = res * H\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(16,32,64,64)\n",
    "t = TAU(a,hidden_dim=3,small_kern=3,dilatation=2,num_layers=3)\n",
    "b = t(a)\n",
    "print(b.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class plTrainingModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(plTrainingModule, self).__init__()\n",
    "        self.model = model\n",
    "        self.MSE = nn.MSELoss()\n",
    "        self.KL = nn.KLDivLoss()\n",
    "        self.tot_loss_tr = []\n",
    "        self.tot_loss = []\n",
    "\n",
    "        self.KL_list = []\n",
    "        self.mse_list = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        ground_truth = batch[1]\n",
    "\n",
    "        ### OUTPUT COMPUTATION ###\n",
    "\n",
    "        prediction = self.model(batch[0])\n",
    "        # KL_Loss = self.compute_KLloss(prediction, ground_truth)\n",
    "        mse = self.MSE(prediction, ground_truth)\n",
    "\n",
    "        loss = mse  # + KL_Loss\n",
    "\n",
    "        self.tot_loss_tr.append(loss.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        ground_truth = batch[1]\n",
    "\n",
    "        ### OUTPUT COMPUTATION ###\n",
    "\n",
    "        prediction = self.model(batch[0])\n",
    "\n",
    "        # KL_Loss = self.compute_KLloss(prediction, ground_truth)\n",
    "\n",
    "        mse = self.MSE(prediction, ground_truth)\n",
    "\n",
    "        loss = mse  # + KL_Loss\n",
    "\n",
    "        self.tot_loss.append(loss.item())\n",
    "        # self.KL_list.append(KL_Loss.item())\n",
    "        self.mse_list.append(mse.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def compute_KLloss(self, predicted, ground_truth):\n",
    "\n",
    "        delta_pred = (predicted[:, 1:] - predicted[:, :-1])/tau\n",
    "        delta_gt = (ground_truth[:, 1:] - ground_truth[:, :-1])/tau\n",
    "\n",
    "        # 3 is the channel-related dimension\n",
    "        soft_pred = F.softmax(delta_pred, dim=3)\n",
    "        # 3 is the channel-related dimension\n",
    "        soft_gt = F.softmax(delta_gt, dim=3)\n",
    "\n",
    "        KL_Loss = self.KL(delta_pred, delta_gt)\n",
    "\n",
    "        return KL_Loss\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if len(self.tot_loss_tr) != 0:\n",
    "            loss_train_mean = sum(self.tot_loss_tr)/len(self.tot_loss_tr)\n",
    "            loss_mean = sum(self.tot_loss)/len(self.tot_loss)\n",
    "            # KL_mean = sum(self.KL_list)/len(self.KL_list)\n",
    "            mse_mean = sum(self.mse_list)/len(self.mse_list)\n",
    "\n",
    "            self.log(name='TOT. Loss on train', value=loss_train_mean)\n",
    "            self.log(name='TOT. Loss on test', value=loss_mean)\n",
    "            # self.log(name = 'KL loss on test', value = KL_mean)\n",
    "            self.log(name='MSE loss on test', value=mse_mean)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr, weight_decay=wd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpu = 1 if torch.cuda.is_available() else 0\n",
    "\n",
    "\n",
    "exp_name = model_conf + ' ' + \\\n",
    "    str(hyperparameters['epochs']) + ' ' + str(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimVP()  # VanillaED(1, 64)\n",
    "\n",
    "\n",
    "pl_training_MDL = plTrainingModule(model)\n",
    "\n",
    "\n",
    "### WANDB CODE ###\n",
    "if wb:\n",
    "\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=project_name, name=exp_name, config=hyperparameters, entity='deepl_wizards')\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=hyperparameters['epochs'],  # maximum number of epochs.\n",
    "        gpus=num_gpu,  # the number of gpus we have at our disposal.\n",
    "        # , overfit_batches = 1\n",
    "        default_root_dir=\"\", logger=wandb_logger, callbacks=[TQDMProgressBar(refresh_rate=20)]\n",
    "    )\n",
    "\n",
    "else:\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=hyperparameters['epochs'],  # maximum number of epochs.\n",
    "        gpus=num_gpu,  # the number of gpus we have at our disposal.\n",
    "        default_root_dir=\"\", callbacks=[TQDMProgressBar(refresh_rate=20)]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=pl_training_MDL, datamodule=data)\n",
    "\n",
    "# WANDB code\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFTMAX ISSUES\n",
    "\n",
    "delta_pred = (output[:, 1:, :, :, :] - output[:, :-1, :, :, :])/tau\n",
    "# delta_pred += 1200\n",
    "delta_gt = (gt[:, 1:, :, :, :] - gt[:, :-1, :, :, :])/tau\n",
    "# delta_gt += 1200\n",
    "soft_pred = F.softmax(delta_pred, dim=3)  # 2 is the channel-related dimension\n",
    "soft_gt = F.softmax(delta_gt, dim=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SimVP(16)\n",
    "load_model('simvp.pt', mod, 'cuda')\n",
    "mod.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(delta_pred.to('cuda'), delta_gt.to('cuda'))\n",
    "torch.equal(soft_pred.to('cuda'), soft_gt.to('cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data.val_dataloader():\n",
    "    gt = batch[1].to('cuda')\n",
    "    output = model(batch[0].to('cuda'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video(gt[1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video(output[1].cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video(batch[0][0].detach())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
