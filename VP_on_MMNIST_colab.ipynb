{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4Fy0j5BTVYd"
      },
      "source": [
        "# Deep Learning course 2022-2023\n",
        "## Project: Video Prediction on Moving MNIST\n",
        "### Project Contributors\n",
        "* Mattia Castelmare, 1815675\n",
        "* Andrea Giuseppe Di Francesco, 1836928\n",
        "* Enrico Fazzi, 2003876"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsRu3F7MTVYf"
      },
      "source": [
        "#### Installation cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJWkmJUPTVYg"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/tychovdo/MovingMNIST.git\n",
        "!pip3 install torchvision\n",
        "!pip3 install matplotlib\n",
        "!pip install wandb\n",
        "!pip3 install torch==1.12.1 torchvision torchaudio torchtext --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "!pip install pytorch-lightning==1.5.10\n",
        "#!pip install pytorch-msssim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xrFoLYJTVYi"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1QdLm0lTVYj"
      },
      "outputs": [],
      "source": [
        "### wandb codes ###\n",
        "import wandb\n",
        "#####################\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
        "from pytorch_lightning import Trainer\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib import rc\n",
        "from MovingMNIST.MovingMNIST import *\n",
        "# from pytorch_msssim import ssim, SSIM\n",
        "rc('animation', html='jshtml')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RzDv39xTVYk"
      },
      "source": [
        "## Useful Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9zDTxbTTVYl"
      },
      "outputs": [],
      "source": [
        "def show_video(tensor):\n",
        "    ''' This function display a video, given a torch tensor (source: https://stackoverflow.com/questions/67261108/how-to-display-a-video-in-colab-using-a-pytorch-tensor-of-rgb-image-arrays)\n",
        "        INPUT: tensor (Frames x Channels x Height x Width) \n",
        "        OUTPUT: Display an animation '''\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    imgs = tensor\n",
        "    imgs = imgs.permute(0, 2, 3, 1)  # Permuting to (Bx)HxWxC format\n",
        "    imgs = imgs.reshape((imgs.shape[0], imgs.shape[1], imgs.shape[2]))\n",
        "    frames = [[ax.imshow(imgs[i], cmap='gray')] for i in range(len(imgs))]\n",
        "\n",
        "    ani = animation.ArtistAnimation(fig, frames)\n",
        "    return ani\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    ''' This function is used within the Dataloader pytorch utility. \n",
        "        INPUT: batch: List of tuples of tensor, each tuple contains the input video and the target video, \n",
        "        OUTPUT: A tuple (input, gt), s.t. both of them has dimension (B x T x C x H x W)'''\n",
        "\n",
        "    list_tuples = list(map(lambda x: (torch.reshape(x[0], (1, x[0].shape[0], 1, x[0].shape[1], x[0].shape[2])), torch.reshape(\n",
        "        x[1], (1, x[1].shape[0], 1, x[1].shape[1], x[1].shape[2]))), batch))\n",
        "\n",
        "    input = list_tuples[0][0]\n",
        "    gt = list_tuples[0][1]\n",
        "\n",
        "    for i in range(1, len(list_tuples)):\n",
        "        input = torch.cat((input, list_tuples[i][0]), dim=0)\n",
        "        gt = torch.cat((gt, list_tuples[i][1]), dim=0)\n",
        "\n",
        "    return ((input/255).type(torch.FloatTensor), (gt/255).type(torch.FloatTensor))\n",
        "\n",
        "\n",
        "def save_model(model, loss, path):\n",
        "  # This function is a customized in order to save a pytorch model.\n",
        "    checkpoint = {'model_state': model.state_dict(),\n",
        "                  'loss': loss}\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "\n",
        "def load_model(path, model):\n",
        "  # This function loads a pytorch model from a path.\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state'])\n",
        "\n",
        "    return checkpoint\n",
        "\n",
        "\n",
        "def stride_generator(N, reverse=False):\n",
        "    ''' This function is desired to know what is the list of stride values to use when defining an E-D architecture, it was taken by https://github.com/gaozhangyang/SimVP-Simpler-yet-Better-Video-Prediction. '''\n",
        "    strides = [1, 2]*10\n",
        "    if reverse:\n",
        "        return list(reversed(strides[:N]))\n",
        "    else:\n",
        "        return strides[:N]\n",
        "\n",
        "\n",
        "def kronecker_product(dyn, static):\n",
        "    \"\"\" This function is a fast implementation of the kronocker product mentioned in the paper https://arxiv.org/pdf/2206.12126.pdf.\n",
        "        The original implementation, from which we took inspiration is available at https://gist.github.com/yulkang/4a597bcc5e9ccf8c7291f8ecb776382d.\n",
        "        INPUTs:\n",
        "            -dyn: results of the dynamical attention module: (B, T*C, 1, 1),\n",
        "            -static: results of the static attention module: (B, T*C, 32, 32).\n",
        "        OUTPUTs:\n",
        "            - res: Kronocker product result.\n",
        "    \"\"\"\n",
        "    siz1 = torch.Size(torch.tensor(\n",
        "        dyn.shape[-2:]) * torch.tensor(static.shape[-2:]))\n",
        "    res = dyn.unsqueeze(-1).unsqueeze(-3) * static.unsqueeze(-2).unsqueeze(-4)\n",
        "    siz0 = res.shape[:-4]\n",
        "    return res.reshape(siz0 + siz1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eyjegjRTVYn"
      },
      "source": [
        "### Useful variables "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioWUjO_3TVYs",
        "outputId": "07c514c7-fe56-4a09-a982-62c44ea891cd"
      },
      "outputs": [],
      "source": [
        "shuffle_train = True\n",
        "shuffle_test = False\n",
        "download = True\n",
        "\n",
        "### wandb codes ###\n",
        "wb = True\n",
        "project_name = \"VP_on_MMNIST\"\n",
        "if wb:\n",
        "    wandb.login()\n",
        "###################\n",
        "\n",
        "SEED = 2812023\n",
        "pl.seed_everything(SEED)\n",
        "\n",
        "root = './data'\n",
        "if not os.path.exists(root):\n",
        "    os.mkdir(root)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaahdolHTVYt"
      },
      "source": [
        "### Pytorch lightning DataModule:\n",
        "* Set the MMNIST Dataset object and Dataloader "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWvZpkxxTVYt"
      },
      "outputs": [],
      "source": [
        "class pl_Dataset(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, batch_size):\n",
        "\n",
        "        self.bs = batch_size\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == 'fit':\n",
        "            self.train_set = MovingMNIST(root=root,\n",
        "                                         train=True,\n",
        "                                         download=download)\n",
        "        elif stage == 'test':\n",
        "            self.test_set = MovingMNIST(root=root,\n",
        "                                        train=False,\n",
        "                                        download=download)\n",
        "\n",
        "    def train_dataloader(self, *args, **kwargs):\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            dataset=self.train_set,\n",
        "            batch_size=self.bs,\n",
        "            shuffle=shuffle_train,\n",
        "            collate_fn=collate)\n",
        "\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self, *args, **kwargs):\n",
        "\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            dataset=self.test_set,\n",
        "            batch_size=self.bs,\n",
        "            shuffle=shuffle_test,\n",
        "            collate_fn=collate)\n",
        "\n",
        "        return test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO8Dx8GSTVYv"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWhRLKp_TVYv",
        "outputId": "79bbe0a9-10ea-4533-b6a8-7ecbb2f5a490"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "lr = 1e-2\n",
        "wd = 5e-2\n",
        "\n",
        "tau = 1e-1 # KL Divergency parameter\n",
        "\n",
        "\n",
        "### Vanilla hyperparameters ###\n",
        "\n",
        "hyperparametersS = {'lr': lr,\n",
        "                    'wd': wd,\n",
        "                    'epochs': 1000,\n",
        "                    'batch_size': batch_size,\n",
        "                    'CNN': {'input': 1,\n",
        "                            'hidden_S': 64,  # Output channels in Encoder / Input channel in Decoder\n",
        "                            'output_T': 512,  # Output channels in Inception, Hidden C. == Output channels // 2\n",
        "                            'ksize': 3,\n",
        "                            'Ns': 4,\n",
        "                            'Nt': 3}\n",
        "                    }\n",
        "\n",
        "hyperparametersT = {'lr': lr,\n",
        "                    'wd': wd,\n",
        "                    'epochs': 100,\n",
        "                    'batch_size': batch_size,\n",
        "                    'CNN': {'input': 1,\n",
        "                            'hidden_S': 64,\n",
        "                            'ksize': 3\n",
        "                            },\n",
        "                    'n_layers': 4\n",
        "                    }\n",
        "\n",
        "\n",
        "data = pl_Dataset(batch_size)\n",
        "\n",
        "data.setup(stage='fit')\n",
        "data.setup(stage='test')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfoqzHOfTVYw"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHngh7AUTVYx"
      },
      "source": [
        "### Our VanillaED (Enry and Matty)\n",
        "* Implementation of a Vanilla Encoder Decoder architecture with a skip connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fSmph3xTVYy"
      },
      "outputs": [],
      "source": [
        "# Some of the following functions are inspired by https://github.com/gaozhangyang/SimVP-Simpler-yet-Better-Video-Prediction.\n",
        "\n",
        "class VConvBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, inputC, outputC, stride=None, padding='same', transpose=False, kernel=hyperparametersT['CNN']['ksize']):\n",
        "        super(VConvBlock, self).__init__()\n",
        "\n",
        "        if not transpose:\n",
        "           self.conv = nn.Conv2d(\n",
        "                    inputC, outputC, kernel_size=kernel, padding=padding, stride=stride)\n",
        "        else:\n",
        "            self.conv = nn.ConvTranspose2d(\n",
        "                inputC, outputC, kernel, stride=stride, padding=1, output_padding=stride // 2)\n",
        "\n",
        "        self.layernorm = nn.GroupNorm(2, outputC)\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = self.layernorm(x)\n",
        "\n",
        "        x = self.activation(x)\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class VEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers):\n",
        "        super(VEncoder, self).__init__()\n",
        "        strides = stride_generator(n_layers)\n",
        "        layers = [VConvBlock(hyperparametersS['CNN']['input'], hyperparametersS['CNN']\n",
        "                            ['hidden_S'], stride=strides[0], padding=1)]\n",
        "\n",
        "        for layer in range(1, n_layers):\n",
        "            stride = strides[layer]\n",
        "            layers.append(VConvBlock(\n",
        "                hyperparametersT['CNN']['hidden_S'], hyperparametersT['CNN']['hidden_S'], stride=stride, padding=1))\n",
        "\n",
        "        self.enc = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C, H, W = x.shape\n",
        "\n",
        "        x = x.reshape((B*T, C, H, W))\n",
        "\n",
        "        skip = self.enc[0](x)\n",
        "        x = self.enc(x)\n",
        "\n",
        "\n",
        "        return x, skip\n",
        "\n",
        "\n",
        "\n",
        "class VDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers):\n",
        "        super(VDecoder, self).__init__()\n",
        "        strides = stride_generator(n_layers, reverse=True)\n",
        "        layers = []\n",
        "        for layer in range(n_layers-1):\n",
        "            stride = strides[layer]\n",
        "            layers.append(VConvBlock(hyperparametersT['CNN']['hidden_S'], hyperparametersT['CNN']\n",
        "                          ['hidden_S'], transpose=True, stride=stride, padding=1))\n",
        "\n",
        "        layers.append(VConvBlock(2*hyperparametersT['CNN']['hidden_S'], hyperparametersT['CNN']\n",
        "                      ['hidden_S'], transpose=True, stride=strides[-1], padding=1))\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            hyperparametersT['CNN']['hidden_S'], hyperparametersS['CNN']['input'], 1)\n",
        "\n",
        "        self.dec = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "\n",
        "        for l_idx in range(len(self.dec)-1):\n",
        "            x = self.dec[l_idx](x)\n",
        "\n",
        "        x = self.dec[-1](torch.cat((x, skip), dim=1))\n",
        "        x = self.proj(x)\n",
        "\n",
        "        return x\n",
        "class VEncoderDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VEncoderDecoder, self).__init__()\n",
        "\n",
        "\n",
        "        self.encoder = VEncoder(hyperparametersS['CNN']['Ns'])\n",
        "        self.decoder = VDecoder(hyperparametersS['CNN']['Ns'])\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        B, T, C, H, W = input.shape\n",
        "        output, skip = self.encoder(input)\n",
        "\n",
        "\n",
        "        output = self.decoder(output, skip)\n",
        "        \n",
        "        output = output.reshape((B, T, C, H, W))\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxg-roCgTVYz"
      },
      "source": [
        "#### SimVP\n",
        "* Implementation of the SimVP architecture, the details are available at https://arxiv.org/pdf/2206.05099.pdf,\n",
        "* Encoder (CNN) +  Translator (Inception) + Decoder (CNN) \n",
        "* The implementation details were taken by the original implementation https://github.com/gaozhangyang/SimVP-Simpler-yet-Better-Video-Prediction , since some of them were neglected in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMpSuzuXTVYz"
      },
      "outputs": [],
      "source": [
        "# Some of the following functions are inspired by https://github.com/gaozhangyang/SimVP-Simpler-yet-Better-Video-Prediction.\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, inputC, outputC, stride=None, padding='same', transpose=False, groups=8, kernel=hyperparametersS['CNN']['ksize'], incep = False):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        if inputC % groups != 0:\n",
        "            groups = 1\n",
        "\n",
        "\n",
        "        if not transpose:\n",
        "            if incep:\n",
        "                self.conv = nn.Conv2d(\n",
        "                    inputC, outputC, kernel_size=kernel, padding=padding, stride=stride, groups = groups)\n",
        "            else:\n",
        "                self.conv = nn.Conv2d(\n",
        "                    inputC, outputC, kernel_size=kernel, padding=padding, stride=stride)\n",
        "        else:\n",
        "            self.conv = nn.ConvTranspose2d(\n",
        "                inputC, outputC, kernel, stride=stride, padding=1, output_padding=stride // 2)\n",
        "\n",
        "        self.layernorm = nn.GroupNorm(groups, outputC)\n",
        "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.leaky(self.layernorm(self.conv(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderSimVP(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers):\n",
        "        super(EncoderSimVP, self).__init__()\n",
        "        strides = stride_generator(n_layers)\n",
        "        layers = [ConvBlock(hyperparametersS['CNN']['input'], hyperparametersS['CNN']\n",
        "                            ['hidden_S'], groups=2, stride=strides[0], padding=1)]\n",
        "\n",
        "        for layer in range(1, n_layers):\n",
        "            stride = strides[layer]\n",
        "            layers.append(ConvBlock(\n",
        "                hyperparametersS['CNN']['hidden_S'], hyperparametersS['CNN']['hidden_S'], groups=2, stride=stride, padding=1))\n",
        "\n",
        "        self.enc = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C, H, W = x.shape\n",
        "\n",
        "        x = x.reshape((B*T, C, H, W))\n",
        "\n",
        "        skip = self.enc[0](x)\n",
        "        x = self.enc(x)\n",
        "\n",
        "        # x = x.reshape((B, T, hyperparameters['CNN']['hidden_S'], H, W))\n",
        "\n",
        "        return x, skip\n",
        "\n",
        "\n",
        "\n",
        "class DecoderSimVP(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers):\n",
        "        super(DecoderSimVP, self).__init__()\n",
        "        strides = stride_generator(n_layers, reverse=True)\n",
        "        layers = []\n",
        "        for layer in range(n_layers-1):\n",
        "            stride = strides[layer]\n",
        "            layers.append(ConvBlock(hyperparametersS['CNN']['hidden_S'], hyperparametersS['CNN']\n",
        "                          ['hidden_S'], transpose=True, groups=2, stride=stride, padding=1))\n",
        "\n",
        "        layers.append(ConvBlock(2*hyperparametersS['CNN']['hidden_S'], hyperparametersS['CNN']\n",
        "                      ['hidden_S'], transpose=True, groups=2, stride=strides[-1], padding=1))\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            hyperparametersS['CNN']['hidden_S'], hyperparametersS['CNN']['input'], 1)\n",
        "\n",
        "        self.dec = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "\n",
        "        for l_idx in range(len(self.dec)-1):\n",
        "            x = self.dec[l_idx](x)\n",
        "\n",
        "        x = self.dec[-1](torch.cat((x, skip), dim=1))\n",
        "        x = self.proj(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "\n",
        "    def __init__(self, inputC, outputC, kernel_list=[3, 5, 7, 11], groups=8):\n",
        "        super(InceptionModule, self).__init__()\n",
        "\n",
        "        hiddenC = outputC // 2\n",
        "\n",
        "        self.Conv = nn.Conv2d(\n",
        "            inputC, hiddenC, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        for kernel in kernel_list:\n",
        "            layers.append(ConvBlock(hiddenC, outputC, groups=groups,\n",
        "                          stride=1, padding=kernel // 2, kernel=kernel, incep = True))\n",
        "\n",
        "        self.ConvParallel = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        x = self.Conv(input)\n",
        "\n",
        "        output = 0\n",
        "\n",
        "        for conv in self.ConvParallel:\n",
        "\n",
        "            output += conv(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "class Translator(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers, inputC):\n",
        "        super(Translator, self).__init__()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        enc_layers = [InceptionModule(\n",
        "            inputC, hyperparametersS['CNN']['output_T'])]\n",
        "\n",
        "        for inc_layer in range(1, n_layers-1):\n",
        "            enc_layers.append(InceptionModule(\n",
        "                hyperparametersS['CNN']['output_T'], hyperparametersS['CNN']['output_T']))\n",
        "\n",
        "        enc_layers.append(InceptionModule(\n",
        "            hyperparametersS['CNN']['output_T'], hyperparametersS['CNN']['output_T']))\n",
        "\n",
        "        self.encoder = nn.Sequential(*enc_layers)\n",
        "\n",
        "        dec_layers = [InceptionModule(\n",
        "            hyperparametersS['CNN']['output_T'], hyperparametersS['CNN']['output_T'])]\n",
        "\n",
        "        for inc_layers in range(1, n_layers-1):\n",
        "            dec_layers.append(InceptionModule(\n",
        "                2*hyperparametersS['CNN']['output_T'], hyperparametersS['CNN']['output_T']))\n",
        "        dec_layers.append(InceptionModule(\n",
        "            2*hyperparametersS['CNN']['output_T'], inputC))\n",
        "\n",
        "        self.decoder = nn.Sequential(*dec_layers)\n",
        "\n",
        "    def forward(self, input):\n",
        "        B, T, C, H, W = input.shape\n",
        "        input = input.reshape((B, T*C, H, W))\n",
        "\n",
        "        enc_feat = []\n",
        "        mid_feat = input\n",
        "\n",
        "        for encoder_layer in self.encoder:\n",
        "            mid_feat = encoder_layer(mid_feat)\n",
        "            enc_feat.append(mid_feat)\n",
        "\n",
        "        output = self.decoder[0](mid_feat)\n",
        "\n",
        "        for l_idx in range(1, self.n_layers):\n",
        "\n",
        "            input = torch.cat((output, enc_feat[-l_idx]), dim=1)\n",
        "            output = self.decoder[l_idx](input)\n",
        "\n",
        "        output = output.reshape((B, T, C, H, W))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class SimVP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimVP, self).__init__()\n",
        "\n",
        "        self.encoder = EncoderSimVP(hyperparametersS['CNN']['Ns'])\n",
        "        self.translator = Translator(\n",
        "            hyperparametersS['CNN']['Nt'], 10*hyperparametersS['CNN']['hidden_S'])\n",
        "        self.decoder = DecoderSimVP(hyperparametersS['CNN']['Ns'])\n",
        "\n",
        "    def forward(self, input):\n",
        "        B, T, C, H, W = input.shape\n",
        "\n",
        "        output, skip = self.encoder(input)\n",
        "        _, C_, H_, W_ = output.shape\n",
        "        output = output.view(B, T, C_, H_, W_)\n",
        "\n",
        "        output = self.translator(output)\n",
        "\n",
        "        output = output.reshape((B*T, C_, H_, W_))\n",
        "\n",
        "        Y = self.decoder(output, skip)\n",
        "\n",
        "        Y = Y.reshape((B, T, C, H, W))\n",
        "\n",
        "        return Y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJCkVHrmTVY3"
      },
      "source": [
        "### TAU Module\n",
        "* Implementation of th Temporal Attention Unit architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ig24nhmNTVY3"
      },
      "outputs": [],
      "source": [
        "''' Static attention parameters where slightly by inspired by the repository available at https://github.com/chengtan9907/SimVPv2.,\n",
        "    Dynamic attention, and squeeze and excitation were partially inspired by https://blog.paperspace.com/channel-attention-squeeze-and-excitation-networks/, but the main implementation derives from the details in the original paper (https://arxiv.org/pdf/1709.01507.pdf) '''\n",
        "class Static_attention(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, kernel_size):\n",
        "        super(Static_attention, self).__init__()\n",
        "      \n",
        "        dilation = 3\n",
        "        self.act = nn.Sigmoid() # if needed\n",
        "        d_k = 2 * dilation - 1\n",
        "        d_p = (d_k - 1) // 2\n",
        "        dd_k = kernel_size // dilation + ((kernel_size // dilation) % 2 - 1)\n",
        "        dd_p = (dilation * (dd_k - 1) // 2)\n",
        "\n",
        "        self.conv0 = nn.Conv2d(dim, dim, d_k, padding=d_p, groups=dim)\n",
        "        self.conv_spatial = nn.Conv2d(dim, dim, dd_k, stride=1, padding=dd_p, groups=dim, dilation=dilation)\n",
        "        self.conv1 = nn.Conv2d(dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(self.conv_spatial(self.conv0(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Dynamic_attention(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, kernel_size):\n",
        "        super(Dynamic_attention, self).__init__()\n",
        "        reduction = 16\n",
        "        self.reduction = max(dim // reduction, 4)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(dim, dim // self.reduction, bias=False), # reduction\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(dim // self.reduction, dim, bias=False), # expansion\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "     \n",
        "    def forward(self, x):\n",
        "        B, _, _, _ = x.shape\n",
        "        x = self.avg_pool(x)\n",
        "        des_shape = x.shape\n",
        "        \n",
        "        x = x.view(B, -1)\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        x = x.reshape(des_shape)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TAU(nn.Module):      # Large Kernel Attention\n",
        "    def __init__(self, dim, kernel_size):\n",
        "        super(TAU, self).__init__()\n",
        "        self.static_net = Static_attention(dim, kernel_size)\n",
        "        self.dynamic_net = Dynamic_attention(dim, kernel_size) \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        stat = self.static_net(x)\n",
        "\n",
        "        dyn = self.dynamic_net(x)\n",
        "\n",
        "        k_prod = kronecker_product(dyn, stat)\n",
        "\n",
        "        out = k_prod * x\n",
        "\n",
        "        return out\n",
        "\n",
        "class TAUNet(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, kernel_size, n_layers = 4):\n",
        "        super(TAUNet, self).__init__()\n",
        "\n",
        "        # self.encoder = VEncoder(input_channels=input_channels, output_channels=output_channels,\n",
        "        #                         kernel_size=kernel_size, n_layers=n_layers)\n",
        "        # self.decoder = VDecoder(input_channels=output_channels, output_channels=input_channels, kernel_size=kernel_size,\n",
        "        #                         n_layers=n_layers)\n",
        "        self.encoder = VEncoder(n_layers)\n",
        "        self.decoder = VDecoder(n_layers)\n",
        "\n",
        "        input_shape = (10*output_channels, 32, 32)\n",
        "        \n",
        "        self.tau = TAU(input_shape[0], kernel_size=21)\n",
        "\n",
        "        self.num_layers = n_layers\n",
        "\n",
        "        self.final = nn.Conv2d(in_channels=2*output_channels, out_channels=input_channels, kernel_size=kernel_size, padding = 'same')\n",
        "\n",
        "    def forward(self, input):\n",
        "        B, T, C, H, W = input.shape\n",
        "        # input = input.reshape((B*T, C, H, W))\n",
        "        output, skip = self.encoder(input)\n",
        "\n",
        "        BT, C_, H_, W_ = output.shape\n",
        "\n",
        "        output = output.reshape((B, T, C_, H_, W_))\n",
        "        output = output.reshape((B, T*C_, H_, W_))\n",
        "\n",
        "        output = self.tau(output)\n",
        "\n",
        "\n",
        "        output = output.reshape((B, T, C_, H_, W_))\n",
        "        output = output.reshape((B*T, C_, H_, W_))\n",
        "\n",
        "        output = self.decoder(output, skip)\n",
        "        \n",
        "        # output = torch.cat((skip, output), dim = 1)\n",
        "        # output = self.final(output)\n",
        "\n",
        "        output = output.reshape((B, T, C, H, W))\n",
        "        \n",
        "        \n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSEx0qAETVY4"
      },
      "source": [
        "# LOSSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkQp8yivTVY5"
      },
      "outputs": [],
      "source": [
        "class MSE_loss(nn.Module):\n",
        "    ''' This class resembles the mean squared error so as defined in the evaluation for the video prediction task. '''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MSE_loss, self).__init__()\n",
        "        self.loss = nn.MSELoss(reduction='sum')\n",
        "\n",
        "    def forward(self, pred, true):\n",
        "        B, T, C, H, W = pred.shape\n",
        "\n",
        "        pred = pred.reshape((B*T, C, H, W))\n",
        "        true = true.reshape((B*T, C, H, W))\n",
        "        loss = []\n",
        "\n",
        "        for i in range(pred.shape[0]):\n",
        "            inter_loss = self.loss(pred[i], true[i])\n",
        "            loss.append(inter_loss)\n",
        "\n",
        "        tot_loss = sum(loss)/len(loss)\n",
        "\n",
        "        return tot_loss\n",
        "\n",
        "\n",
        "class plTrainingModule(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model, model_conf, path=None, save=False, load=False):\n",
        "        super(plTrainingModule, self).__init__()\n",
        "        self.model = model\n",
        "        self.mc = model_conf\n",
        "        self.save = save\n",
        "        self.load = load\n",
        "        self.dest_path = path\n",
        "\n",
        "        self.MSE = MSE_loss()\n",
        "        if self.mc == 'tau':\n",
        "            self.KL = nn.KLDivLoss()\n",
        "\n",
        "        self.KL_list = []\n",
        "        self.tot_loss_tr = []\n",
        "        self.tot_loss = []\n",
        "        self.mse_list = []\n",
        "\n",
        "        if self.load:\n",
        "            load_model(self.dest_path, self.model)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "\n",
        "        ground_truth = batch[1]\n",
        "\n",
        "        ### OUTPUT COMPUTATION ###\n",
        "\n",
        "        prediction = self.model(batch[0])\n",
        "        mse = self.MSE(prediction, ground_truth)\n",
        "\n",
        "        loss = mse\n",
        "\n",
        "        if self.mc == 'tau':\n",
        "            KL_Loss = self.compute_KLloss(prediction, ground_truth)\n",
        " \n",
        "            loss += KL_Loss\n",
        "\n",
        "        self.tot_loss_tr.append(loss.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "\n",
        "        ground_truth = batch[1]\n",
        "\n",
        "        ### OUTPUT COMPUTATION ###\n",
        "\n",
        "        prediction = self.model(batch[0])\n",
        "        mse = self.MSE(prediction, ground_truth)\n",
        "\n",
        "        loss = mse\n",
        "\n",
        "        if self.mc == 'tau':\n",
        "            KL_Loss = self.compute_KLloss(prediction, ground_truth)\n",
        "            loss += KL_Loss\n",
        "            self.KL_list.append(KL_Loss.item())\n",
        "\n",
        "        self.tot_loss.append(loss.item())\n",
        "\n",
        "        self.mse_list.append(mse.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def compute_KLloss(self, predicted, ground_truth):\n",
        "        loss = []\n",
        "        B, T, C, H, W = predicted.shape\n",
        "        B, T, C, H, W = ground_truth.shape\n",
        "\n",
        "        pred = predicted.reshape((B*T, C, H, W))\n",
        "        ground = ground_truth.reshape((B*T, C, H, W))\n",
        "\n",
        "        delta_pred = (pred[1:] - pred[:-1])/tau\n",
        "        delta_gt = (ground[1:] - ground[:-1])/tau\n",
        "\n",
        "        # 2 is the channel-related dimension\n",
        "        soft_pred = F.softmax(delta_pred, dim=2)\n",
        "        # 2 is the channel-related dimension\n",
        "        soft_gt = F.softmax(delta_gt, dim=2)\n",
        "\n",
        "        KL_Loss = self.KL(delta_pred, delta_gt)\n",
        "        loss.append(KL_Loss)\n",
        "        tot_loss = sum(loss)/len(loss)\n",
        "\n",
        "        return KL_Loss\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if len(self.tot_loss_tr) != 0:\n",
        "            loss_train_mean = sum(self.tot_loss_tr)/len(self.tot_loss_tr)\n",
        "            loss_mean = sum(self.tot_loss)/len(self.tot_loss)\n",
        "\n",
        "            mse_mean = sum(self.mse_list)/len(self.mse_list)\n",
        "\n",
        "            self.log(name='TOT. Loss on train', value=loss_train_mean,\n",
        "                     on_epoch=True, prog_bar=True, logger=True)\n",
        "            self.log(name='TOT. Loss on test', value=loss_mean,\n",
        "                     on_epoch=True, prog_bar=True, logger=True)\n",
        "            self.log(name='MSE loss on test', value=mse_mean,\n",
        "                     on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "            if self.mc == 'tau':\n",
        "                KL_mean = sum(self.KL_list)/len(self.KL_list)\n",
        "                self.log(name='KL loss on test', value=KL_mean,\n",
        "                         on_epoch=True, prog_bar=True, logger=True)\n",
        "                self.KL_list = []\n",
        "            self.tot_loss_tr = []\n",
        "            self.tot_loss = []\n",
        "            self.mse_list = []\n",
        "\n",
        "            if self.save:\n",
        "\n",
        "                save_model(self.model, loss_mean, self.dest_path)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.model.parameters(), lr, weight_decay=wd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjtNkENJTVY6"
      },
      "outputs": [],
      "source": [
        "num_gpu = 1 if torch.cuda.is_available() else 0\n",
        "\n",
        "\n",
        "save = True\n",
        "load = False\n",
        "\n",
        "\n",
        "model_conf = 'tau'\n",
        "\n",
        "if model_conf == 'tau' or model_conf == 'random':\n",
        "    hyperparameters = hyperparametersT\n",
        "elif model_conf == 'simvp':\n",
        "    hyperparameters = hyperparametersS\n",
        "\n",
        "exp_name = 'MSE_V_ED' + model_conf + '_M_' + \\\n",
        "    str(hyperparameters['epochs']) + '_' + str(batch_size)\n",
        "\n",
        "path = exp_name + '.pt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEJEWy50TVY7",
        "outputId": "50072283-ea92-49c7-f78e-e364174e394f"
      },
      "outputs": [],
      "source": [
        "if model_conf == 'tau':\n",
        "    model = TAUNet(hyperparameters['CNN']['input'], hyperparameters['CNN']\n",
        "                   ['hidden_S'], hyperparameters['CNN']['ksize'], hyperparameters['n_layers'])\n",
        "elif model_conf == 'random':\n",
        "    model = VEncoderDecoder()\n",
        "elif model_conf == 'simvp':\n",
        "    model = SimVP()\n",
        "\n",
        "\n",
        "pl_training_MDL = plTrainingModule(\n",
        "    model, model_conf, path, save=save, load=load)\n",
        "\n",
        "\n",
        "### WANDB CODE ###\n",
        "if wb:\n",
        "\n",
        "    wandb_logger = WandbLogger(\n",
        "        project=project_name, name=exp_name, config=hyperparameters, entity='deepl_wizards')\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=hyperparameters['epochs'],  # maximum number of epochs.\n",
        "        gpus=num_gpu,  # the number of gpus we have at our disposal.\n",
        "        # , overfit_batches = 1\n",
        "        default_root_dir=\"\", logger=wandb_logger, callbacks=[TQDMProgressBar(refresh_rate=20)], deterministic = True\n",
        "    )\n",
        "\n",
        "else:\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=hyperparameters['epochs'],  # maximum number of epochs.\n",
        "        gpus=num_gpu,  # the number of gpus we have at our disposal.\n",
        "        default_root_dir=\"\", callbacks=[TQDMProgressBar(refresh_rate=20)], deterministic = True\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399,
          "referenced_widgets": [
            "19acc25602f441bd9560e89c4c6349b0",
            "cd1e400b31c942b287a3e38fe495c2b2",
            "486ed970f53148968d996a0103953c8d",
            "2068c4902e294b59a2ffe512e2f9ce71",
            "224d4f1ec2a94be5ac62660d8f41c7bf",
            "d5dba97d8c4f464dbd01fe7b0a020d16",
            "99e7c43b507041cd9d359956e06d2752",
            "50f1facf11e2428494247b911122833a",
            "0d4da70e65e447bbbbdc7466a00362e0",
            "4c0b7d6203774103a41868d7564467ed",
            "965c026ceb5a46ad816169220da60fe8",
            "6c13bfee576b494aa8bc003091f4d662",
            "7aad82babb0a4ef8b01012517e585b33",
            "9850158831ba4389a693d9d5d158ef01",
            "804e1c6488cf4ed4bab483d7d826826b",
            "2157b603d95d49adb8eabddf6ab1d802",
            "9495470dc3984dc48bb853df1037ebf2",
            "40e0db8d52204661a6df6dda694b10e7",
            "560b2e0b77234708b643d9b1f8814bd1",
            "fcc19a3cec8b49e788e18f56d62d4909",
            "e0c370f629e14b9dbd02d0c617641656",
            "02016cb573354a4685fddaa424df89b4"
          ]
        },
        "id": "06LQw-qaTVY7",
        "outputId": "3c0cef7f-ecb1-4385-fe2d-1da46055eb4c"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model=pl_training_MDL, datamodule=data)\n",
        "\n",
        "# WANDB code\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOi8t40FTVY8"
      },
      "outputs": [],
      "source": [
        "loss = MSE_loss()\n",
        "loss_list = []\n",
        "c = 0\n",
        "model.to('cuda')\n",
        "\n",
        "for batch in data.val_dataloader():\n",
        "    if c == 1:\n",
        "        break\n",
        "    input = batch[0].to('cuda')\n",
        "    gt = batch[1].to('cuda')\n",
        "    output = model(batch[0].to('cuda'))\n",
        "\n",
        "    inter_loss = loss(output, input)\n",
        "    loss_list.append(inter_loss)\n",
        "\n",
        "    c += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsQTwFFITVY9"
      },
      "outputs": [],
      "source": [
        "load_model('simvp_1000.pt', model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOObRQpDTVY9",
        "outputId": "9843791b-bdd2-4893-d8c7-26ac86822e66"
      },
      "outputs": [],
      "source": [
        "loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBtN-6b4TVY-",
        "outputId": "5706c0e5-3330-47fe-a1d3-23428961f78a"
      },
      "outputs": [],
      "source": [
        "show_video(gt[2].cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFzigwprTVY_",
        "outputId": "42d79f7f-288f-4204-bc6e-265693d6bc9b"
      },
      "outputs": [],
      "source": [
        "show_video(output[2].cpu().detach())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXj-obKQTVY_"
      },
      "outputs": [],
      "source": [
        "show_video(batch[0][0].detach())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "my_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6 (main, Oct  7 2022, 20:19:58) [GCC 11.2.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "3d541b27ed3f6b3a2668175b94af942c53fef9dc2000d6033c45f742df07c856"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02016cb573354a4685fddaa424df89b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d4da70e65e447bbbbdc7466a00362e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19acc25602f441bd9560e89c4c6349b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd1e400b31c942b287a3e38fe495c2b2",
              "IPY_MODEL_486ed970f53148968d996a0103953c8d",
              "IPY_MODEL_2068c4902e294b59a2ffe512e2f9ce71"
            ],
            "layout": "IPY_MODEL_224d4f1ec2a94be5ac62660d8f41c7bf"
          }
        },
        "2068c4902e294b59a2ffe512e2f9ce71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c0b7d6203774103a41868d7564467ed",
            "placeholder": "​",
            "style": "IPY_MODEL_965c026ceb5a46ad816169220da60fe8",
            "value": " 2/2 [00:00&lt;00:00,  6.73it/s]"
          }
        },
        "2157b603d95d49adb8eabddf6ab1d802": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "224d4f1ec2a94be5ac62660d8f41c7bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "40e0db8d52204661a6df6dda694b10e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "486ed970f53148968d996a0103953c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50f1facf11e2428494247b911122833a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d4da70e65e447bbbbdc7466a00362e0",
            "value": 2
          }
        },
        "4c0b7d6203774103a41868d7564467ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50f1facf11e2428494247b911122833a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "560b2e0b77234708b643d9b1f8814bd1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c13bfee576b494aa8bc003091f4d662": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7aad82babb0a4ef8b01012517e585b33",
              "IPY_MODEL_9850158831ba4389a693d9d5d158ef01",
              "IPY_MODEL_804e1c6488cf4ed4bab483d7d826826b"
            ],
            "layout": "IPY_MODEL_2157b603d95d49adb8eabddf6ab1d802"
          }
        },
        "7aad82babb0a4ef8b01012517e585b33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9495470dc3984dc48bb853df1037ebf2",
            "placeholder": "​",
            "style": "IPY_MODEL_40e0db8d52204661a6df6dda694b10e7",
            "value": "Epoch 0:   3%"
          }
        },
        "804e1c6488cf4ed4bab483d7d826826b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0c370f629e14b9dbd02d0c617641656",
            "placeholder": "​",
            "style": "IPY_MODEL_02016cb573354a4685fddaa424df89b4",
            "value": " 20/626 [00:04&lt;02:26,  4.13it/s, loss=1.28e+03, v_num=3]"
          }
        },
        "9495470dc3984dc48bb853df1037ebf2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "965c026ceb5a46ad816169220da60fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9850158831ba4389a693d9d5d158ef01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_560b2e0b77234708b643d9b1f8814bd1",
            "max": 626,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcc19a3cec8b49e788e18f56d62d4909",
            "value": 20
          }
        },
        "99e7c43b507041cd9d359956e06d2752": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd1e400b31c942b287a3e38fe495c2b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5dba97d8c4f464dbd01fe7b0a020d16",
            "placeholder": "​",
            "style": "IPY_MODEL_99e7c43b507041cd9d359956e06d2752",
            "value": "Validation sanity check: 100%"
          }
        },
        "d5dba97d8c4f464dbd01fe7b0a020d16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0c370f629e14b9dbd02d0c617641656": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcc19a3cec8b49e788e18f56d62d4909": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
